{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e74e7013",
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORT EXCEPTION MODULES\n",
    "from requests.exceptions import Timeout\n",
    "from github import GithubException, UnknownObjectException, IncompletableObject\n",
    "\n",
    "### IMPORT SYSTEM MODULES\n",
    "from github import Github\n",
    "import os, logging, pandas, csv, tempfile, shutil\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from truckfactor.compute import main as compute_tf\n",
    "import re\n",
    "import unicodedata\n",
    "from typing import Iterable, Tuple, List, Set, Dict, List\n",
    "from collections import Counter\n",
    "\n",
    "### IMPORT CUSTOM MODULES\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import Settings as cfg\n",
    "import Utilities as util\n",
    "import subprocess, tempfile, shutil\n",
    "\n",
    "from git import Repo, exc as git_exc\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e559df91",
   "metadata": {},
   "source": [
    "# TESTER FOR BREAK IDENTIFICATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c9ce505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_pauses_table(\n",
    "        df: pandas.DataFrame,\n",
    "        out_path: os.PathLike,\n",
    "        authors: list[str] | None = None,\n",
    "        *,\n",
    "        user_col: str = \"author_id\",\n",
    "        date_col: str = \"created_at\",\n",
    "        tail_to_today: bool = False\n",
    "    ) -> pandas.DataFrame:\n",
    "\n",
    "    df[date_col] = pandas.to_datetime(df[date_col]).dt.normalize()\n",
    "\n",
    "    if authors is None:\n",
    "        authors = df[user_col].unique()\n",
    "\n",
    "    rows = []\n",
    "    \n",
    "    count =0\n",
    "    for dev in authors:\n",
    "        user_df = df[df[user_col] == dev]\n",
    "        pause_len_1 = len(user_df[date_col].dt.date.unique())\n",
    "        pause_len_2 =len(user_df)\n",
    "        if user_df.empty:\n",
    "            continue\n",
    "\n",
    "        active_days = sorted(user_df[date_col].dt.date.unique())\n",
    "        current_row = [dev]\n",
    "\n",
    "        for i in range(len(active_days) - 1):\n",
    "            prev_day = active_days[i]\n",
    "            next_day = active_days[i + 1]\n",
    "            gap = (next_day - prev_day).days\n",
    "            if gap > 1:\n",
    "                # Inactivity starts the day after prev_day\n",
    "                current_row.append(f\"{(prev_day + pandas.Timedelta(days=1)).strftime('%Y-%m-%d')}/{next_day.strftime('%Y-%m-%d')}\")\n",
    "            else:\n",
    "                count += 1\n",
    "\n",
    "\n",
    "        if tail_to_today and active_days:\n",
    "            today = _date.today()\n",
    "            gap = (today - active_days[-1]).days\n",
    "            if gap > 1:\n",
    "                current_row.append(f\"{active_days[-1]}/{today}\")\n",
    "\n",
    "        if len(current_row) > 1:\n",
    "            rows.append(current_row)\n",
    "    \n",
    "    out_path = Path(out_path)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with out_path.open(\"w\", newline=\"\",encoding=\"utf-8\" ) as f:\n",
    "        csv.writer(f, delimiter=\",\", quoting=csv.QUOTE_NONE).writerows(rows)\n",
    "\n",
    "    return pandas.DataFrame(rows)\n",
    "\n",
    "def get_commit_based_core_devs(commits, threshold=0.8):\n",
    "    \"\"\"\n",
    "    commits: List[dict] where each dict contains at least the 'author' key.\n",
    "    Example: [{'author': 'alice'}, {'author': 'bob'}, {'author': 'alice'}, ...]\n",
    "\n",
    "    Returns: List of core developers (author names) who together authored >= threshold of commits.\n",
    "    \"\"\"\n",
    "    # Count commits per developer\n",
    "    author_commit_counts = Counter(commit[\"author_id\"] for commit in commits)\n",
    "\n",
    "    # Sort developers by number of commits (descending)\n",
    "    sorted_authors = author_commit_counts.most_common()\n",
    "\n",
    "    total_commits = sum(author_commit_counts.values())\n",
    "    cumulative = 0\n",
    "    core_devs = []\n",
    "\n",
    "    for author, count in sorted_authors:\n",
    "        cumulative += count\n",
    "        core_devs.append(author)\n",
    "        if cumulative / total_commits >= threshold:\n",
    "            break\n",
    "\n",
    "    return core_devs\n",
    "\n",
    "def identifyInactivityPeriods(organizationFolder, organization, project):\n",
    "    \"\"\"Identifies the inactivity periods of the developers in the organization\"\"\"\n",
    "    #url = \"https://github.com/\" + organization + \"/\" + project + \".git\"\n",
    "    #authors, emails = findCoreDevelopers(url, name=project)\n",
    "    \n",
    "    organizationFolder = organizationFolder + \"/\" + organization + \"/\" + project\n",
    "\n",
    "    commits =  pandas.read_csv(organizationFolder + \"/commit_list.csv\", parse_dates=[\"created_at\"], encoding=\"utf-8\", header=0, sep=cfg.CSV_separator)\n",
    "\n",
    "    commit_authors = get_commit_based_core_devs(commits.to_dict(orient='records'))\n",
    "\n",
    "    pauses = write_pauses_table(commits, organizationFolder + \"/pauses_commits.csv\", commit_authors, user_col = \"author_id\", date_col=\"created_at\")\n",
    "    return commit_authors, pauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dec270e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFarOutThreshold(values, dev): ### If it is satisfying, move the function into UTILITIES\n",
    "    import numpy\n",
    "    th = 0\n",
    "    q_3rd = numpy.percentile(values,75)\n",
    "    q_1st = numpy.percentile(values,25)\n",
    "    iqr = q_3rd-q_1st\n",
    "    if iqr > 1:\n",
    "        th = q_3rd + 3*iqr\n",
    "    return th\n",
    "\n",
    "def addToBreaksList(pauses, currentBreaks, th):\n",
    "    for _, p in pauses.iterrows():\n",
    "        if (p['len'] > th) and (p['dates'] not in currentBreaks.dates.tolist()):\n",
    "            util.add(currentBreaks, [p['len'], p['dates'], th])\n",
    "    return currentBreaks\n",
    "\n",
    "def cleanClearBreaks(clearBreaks, breaks):\n",
    "    for _, b in breaks.iterrows():\n",
    "        clearBreaks = clearBreaks[clearBreaks.dates != b['dates']] # If it was in the long_breaks list, remove ot from there\n",
    "    return clearBreaks\n",
    "\n",
    "def identifyBreaks(pauses_dates_list, developer, window, shift,\n",
    "                   debug_folder=None):           # NEW ARG\n",
    "    '''\n",
    "    Removes SURE BREAKS from windows to calculate Tfov\n",
    "    and — with debug_folder — writes a per-window diagnostics CSV.\n",
    "    '''\n",
    "    breaks_df = pandas.DataFrame(columns=['len', 'dates', 'th'])\n",
    "    diagnostics = []                             # NEW\n",
    "\n",
    "    for row in pauses_dates_list:\n",
    "\n",
    "        dev = developer\n",
    "        intervals_list = [ x for x in row[1:]\n",
    "                          if isinstance(x, str) and '/' in x and x.strip()]\n",
    "        \n",
    "        if not intervals_list:\n",
    "            print(dev, 'has NO valid pauses')\n",
    "            continue                      # <- don’t bail out; just skip\n",
    "\n",
    "        clear_breaks = pandas.DataFrame(columns=['len', 'dates'])\n",
    "\n",
    "        FPS_dt = datetime.strptime(intervals_list[0].split('/')[0], '%Y-%m-%d')\n",
    "        LPE_dt = datetime.strptime(intervals_list[-1].split('/')[1], '%Y-%m-%d')\n",
    "\n",
    "        win_start, win_end = FPS_dt, FPS_dt + timedelta(days=window)\n",
    "        last_th = 0\n",
    "        while win_end < LPE_dt:\n",
    "            win_pauses_list = pandas.DataFrame(columns=['len', 'dates'])\n",
    "            partially_included_pauses_list = pandas.DataFrame(columns=['len', 'dates'])\n",
    "\n",
    "            for interval in intervals_list:\n",
    "                int_start_str, int_end_str = interval.split('/')          # keep strings\n",
    "                int_start_dt  = datetime.strptime(int_start_str, '%Y-%m-%d')\n",
    "                int_end_dt    = datetime.strptime(int_end_str,   '%Y-%m-%d')\n",
    "                pause_len = util.daysBetween(int_start_str, int_end_str)\n",
    "                # fully inside\n",
    "                if int_start_dt >= win_start and int_end_dt <= win_end:\n",
    "                    util.add(win_pauses_list, [pause_len, interval])\n",
    "                # touches boundary\n",
    "                if ((int_start_dt <= win_end and int_end_dt > win_end) or\n",
    "                    (int_end_dt >= win_start and int_start_dt < win_start)):\n",
    "                    util.add(partially_included_pauses_list, [pause_len, interval])\n",
    "\n",
    "            win_pauses = len(win_pauses_list)\n",
    "            pauses = pandas.concat([win_pauses_list,\n",
    "                                    partially_included_pauses_list],\n",
    "                                    ignore_index=True)\n",
    "\n",
    "            # --- decision logic (unchanged) ---------------------------------\n",
    "            win_th = None\n",
    "            added_flag = False\n",
    "            if win_pauses >= 4:\n",
    "                win_th = getFarOutThreshold(win_pauses_list['len'], dev)\n",
    "                if win_th > 0:\n",
    "                    before = len(breaks_df)\n",
    "                    breaks_df = addToBreaksList(pauses, breaks_df, win_th)\n",
    "                    added_flag = len(breaks_df) > before\n",
    "                    last_th = win_th\n",
    "                elif last_th > 0:\n",
    "                    before = len(breaks_df)\n",
    "                    breaks_df = addToBreaksList(pauses, breaks_df, last_th)\n",
    "                    added_flag = len(breaks_df) > before\n",
    "            else:\n",
    "                if last_th > 0:\n",
    "                    before = len(breaks_df)\n",
    "                    breaks_df = addToBreaksList(pauses, breaks_df, last_th)\n",
    "                    added_flag = len(breaks_df) > before\n",
    "\n",
    "                clear_breaks = cleanClearBreaks(clear_breaks, breaks_df)\n",
    "                for _, p in pauses.iterrows():\n",
    "                    if (p['len'] >= window and\n",
    "                        p['dates'] not in clear_breaks.dates.tolist() and\n",
    "                        p['dates'] not in breaks_df.dates.tolist()):\n",
    "                        util.add(clear_breaks, p)\n",
    "\n",
    "            # ----------- NEW: record diagnostics for this window -------------\n",
    "            diagnostics.append({\n",
    "                'win_start': win_start.date(),\n",
    "                'win_end':   win_end.date(),\n",
    "                'win_pauses': win_pauses,\n",
    "                'pause_lengths': ';'.join(map(str, win_pauses_list['len'].tolist())),\n",
    "                'partial_lengths': ';'.join(map(str, partially_included_pauses_list['len'].tolist())),\n",
    "                'win_th': win_th,\n",
    "                'last_th': last_th,\n",
    "                'added_as_break': 'yes' if added_flag else 'no'\n",
    "            })\n",
    "            # -----------------------------------------------------------------\n",
    "\n",
    "            win_start += timedelta(days=shift)\n",
    "            win_end   = win_start + timedelta(days=window)\n",
    "\n",
    "\n",
    "    return breaks_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c727774c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_activity_csv(folder: str,\n",
    "                       filename: str,\n",
    "                       rename_map: Dict[str, str],\n",
    "                       dev_login,\n",
    "                       usecols: list[str] = None,\n",
    "                       ) -> pandas.DataFrame:\n",
    "    \"\"\"\n",
    "    Read *filename* in *folder*, rename to the canonical columns\n",
    "    ('id','date','creator_login'), keep ONLY the specified dev, and\n",
    "    return three columns.  On any problem → empty df.\n",
    "    \"\"\"\n",
    "    path = os.path.join(folder, filename)\n",
    "    try:\n",
    "        df = pandas.read_csv(path, sep=cfg.CSV_separator, usecols=usecols)\n",
    "    except FileNotFoundError:\n",
    "        logging.info(\"File %s not found – skipping\", path)\n",
    "        return pandas.DataFrame(columns=[\"id\", \"date\", \"creator_login\"])\n",
    "    except Exception as e:\n",
    "        logging.warning(\"Could not read %s: %s\", path, e)\n",
    "        return pandas.DataFrame(columns=[\"id\", \"date\", \"creator_login\"])\n",
    "\n",
    "    df = df.rename(columns=rename_map)\n",
    "    # keep only the columns we need, ignore anything extra\n",
    "    df = df[[\"id\", \"date\", \"creator_login\"]]\n",
    "    df = df[df.creator_login == dev_login]\n",
    "    # allow str OR list[str]\n",
    "    if isinstance(dev_login, list):\n",
    "        df = df[df.creator_login.isin(dev_login)]\n",
    "    else:\n",
    "        df = df[df.creator_login == dev_login]\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def get_activities(folder: str, dev_login: str) -> pandas.DataFrame:\n",
    "    \"\"\"\n",
    "    Build the developer's DAILY 'other-actions' table.\n",
    "    Returns a dataframe whose index is the *action*\n",
    "    ('issues/pull_requests', 'issues_comments', …) and whose\n",
    "    columns are day-strings.\n",
    "    \"\"\"\n",
    "    files = {\n",
    "    \"prs\": (\n",
    "        \"prs_repo.csv\",\n",
    "        {\"PR_id\": \"id\", \"created_at\": \"date\", \"created_by\": \"creator_login\"},\n",
    "    ),\n",
    "    \"prs_comments\": (\n",
    "        \"prs_comments.csv\",\n",
    "        {\"comment_id\": \"id\", \"created_at\": \"date\", \"created_by\": \"creator_login\"},\n",
    "    ),\n",
    "    \"issues\": (\n",
    "        \"issues_repo.csv\",\n",
    "        {\"issue_id\": \"id\", \"created_at\": \"date\", \"created_by\": \"creator_login\"},\n",
    "    ),\n",
    "    \"issues_comments\": (\n",
    "        \"issues_comments_repo.csv\",\n",
    "        {\"comment_id\": \"id\", \"created_at\": \"date\", \"created_by\": \"creator_login\"},\n",
    "    ),\n",
    "    \"issues_events\": (\n",
    "        \"issues_events_repo.csv\",\n",
    "        {\"event_id\": \"id\", \"created_at\": \"date\", \"created_by\": \"creator_login\"},\n",
    "    ),\n",
    "    \"issues_timeline\": (\n",
    "        \"issues_timeline_repo.csv\",\n",
    "        {\"event_id\": \"id\", \"created_at\": \"date\", \"created_by\": \"creator_login\"},\n",
    "    )\n",
    "    }\n",
    "\n",
    "    # ---------- read / filter every file ----------\n",
    "    dfs = {}\n",
    "    for key, (fname, rename_map) in files.items():\n",
    "        dfs[key] = _load_activity_csv(folder, fname, rename_map, dev_login)\n",
    "\n",
    "    # ---------- split issues vs PRs -------------\n",
    "    # Old logic: issues endpoint also returns PRs; remove rows whose id\n",
    "    # matches a PR id so we don’t double-count.\n",
    "    if not dfs[\"issues\"].empty and not dfs[\"prs\"].empty:\n",
    "        dfs[\"issues\"] = dfs[\"issues\"][~dfs[\"issues\"].id.isin(dfs[\"prs\"].id)]\n",
    "\n",
    "    # ---------- build the day range -------------\n",
    "    # Derive it from the *actual* activity we just read.\n",
    "    #\n",
    "    # 1) gather every non-empty dataframe\n",
    "    non_empty = [df for df in dfs.values() if not df.empty]\n",
    "\n",
    "    if non_empty:\n",
    "        # 2) earliest / latest date across *all* action types\n",
    "        min_date = min(df[\"date\"].min() for df in non_empty)\n",
    "        max_date = max(df[\"date\"].max() for df in non_empty)\n",
    "    else:\n",
    "        # Developer has no activity at all → default to one-day range\n",
    "        min_date = max_date = pandas.Timestamp.today()\n",
    "\n",
    "    # 3) full, dense list of day strings\n",
    "    day_cols = (\n",
    "        pandas.date_range(\n",
    "            start=pandas.to_datetime(min_date).normalize(),\n",
    "            end=pandas.to_datetime(max_date).normalize(),\n",
    "            freq=\"D\",\n",
    "        )\n",
    "        .strftime(\"%Y-%m-%d\")\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    # ---------- helper to create one timeline row ----------\n",
    "    def _timeline_row(action_name, df_raw):\n",
    "        row = [action_name]\n",
    "        if df_raw.empty:\n",
    "            row += [0] * len(day_cols)\n",
    "            return row\n",
    "        counts = (\n",
    "            pandas.to_datetime(df_raw[\"date\"])\n",
    "            .dt.date\n",
    "            .value_counts()\n",
    "            .to_dict()\n",
    "        )\n",
    "        for d in day_cols:\n",
    "            row.append(counts.get(pandas.to_datetime(d).date(), 0))\n",
    "        return row\n",
    "\n",
    "    # ---------- compile all action rows ----------\n",
    "    rows = []\n",
    "    if not dfs[\"issues\"].empty:\n",
    "        rows.append(_timeline_row(\"issues\", dfs[\"issues\"]))\n",
    "    if not dfs[\"issues_comments\"].empty:\n",
    "        rows.append(_timeline_row(\"issues_comments\", dfs[\"issues_comments\"]))\n",
    "    if not dfs[\"issues_events\"].empty:\n",
    "        rows.append(_timeline_row(\"issues_events\", dfs[\"issues_events\"]))\n",
    "    if not dfs[\"prs\"].empty:\n",
    "        rows.append(_timeline_row(\"pull_requests\", dfs[\"prs\"]))\n",
    "    if not dfs[\"prs_comments\"].empty:\n",
    "        rows.append(_timeline_row(\"pull_requests_comments\", dfs[\"prs_comments\"]))\n",
    "\n",
    "    # (commits are already encoded in coding_history_table, so we skip them here)\n",
    "\n",
    "    actions = pandas.DataFrame(rows, columns=[\"action\"] + day_cols).set_index(\"action\")\n",
    "\n",
    "    # ---------- cache to disk, same place as before ----------\n",
    "    actions_folder = Path(folder) / cfg.actions_folder_name\n",
    "    actions_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"actions folder:\", actions_folder)\n",
    "    os.makedirs(actions_folder, exist_ok=True)\n",
    "    actions_file = actions_folder / f\"{dev_login}_actions_table.csv\"\n",
    "    print(\"actions file:\", actions_file)\n",
    "    actions.to_csv( actions_file , sep=cfg.CSV_separator, na_rep=cfg.CSV_missing, index=False)\n",
    "    return actions\n",
    "\n",
    "def splitBreak(break_limits, action_days, th):\n",
    "    status = 'ACTIVE'  # NCUT: Non coding under threshold.\n",
    "    previously = status\n",
    "    period_start = ''\n",
    "\n",
    "    break_range = break_limits.split('/')\n",
    "    action_days.insert(0, break_range[0])\n",
    "    action_days.append(break_range[1])\n",
    "\n",
    "    period_detail = pandas.DataFrame(columns=['len', 'dates', 'th', 'label', 'previously'])\n",
    "    for i in range(0, len(action_days) - 1):\n",
    "        if status == 'ACTIVE':\n",
    "            previously = status\n",
    "            size = util.daysBetween(action_days[i], action_days[i + 1])\n",
    "            if size > th:\n",
    "                if size > cfg.gone_threshold:\n",
    "                    status = 'GONE'\n",
    "                else:\n",
    "                    status = 'INACTIVE'\n",
    "                dates = action_days[i] + '/' + action_days[i + 1]\n",
    "                util.add(period_detail, [size, dates, th, status, previously])\n",
    "            else:\n",
    "                status = 'NCUT'\n",
    "                period_start = action_days[i]\n",
    "        elif (status == 'INACTIVE') | (status == 'GONE'):\n",
    "            previously = status\n",
    "            size = util.daysBetween(action_days[i], action_days[i + 1])\n",
    "            if size < th:\n",
    "                status = 'NCUT'\n",
    "                period_start = action_days[i]\n",
    "            else:\n",
    "                residual = size - (th + 1)\n",
    "                if residual > th:\n",
    "                    # The sub-break is actually made of 2 breaks: Non-coding + Inactive/Gone\n",
    "                    status = 'NON_CODING'\n",
    "                    final_date = (datetime.strptime(action_days[i], \"%Y-%m-%d\") + timedelta(days=(th + 1))).strftime(\"%Y-%m-%d\")\n",
    "                    dates = action_days[i] + '/' + final_date\n",
    "                    actual_size = util.daysBetween(action_days[i], final_date)\n",
    "                    util.add(period_detail, [actual_size, dates, th, status, previously])\n",
    "\n",
    "                    previously = status\n",
    "                    if residual > cfg.gone_threshold:\n",
    "                        status = 'GONE'\n",
    "                    else:\n",
    "                        status = 'INACTIVE'\n",
    "                    dates = final_date + '/' + action_days[i + 1]\n",
    "                    second_size = util.daysBetween(final_date, action_days[i + 1])\n",
    "                    util.add(period_detail, [second_size, dates, th, status, previously])\n",
    "                else:\n",
    "                    # The sub-break becomes a Non-coding\n",
    "                    status = 'NON_CODING'\n",
    "                    dates = action_days[i] + '/' + action_days[i + 1]\n",
    "                    util.add(period_detail, [size, dates, th, status, previously])\n",
    "        elif status == 'NON_CODING':\n",
    "            previously = status\n",
    "            size = util.daysBetween(action_days[i], action_days[i + 1])\n",
    "            if size > th:\n",
    "                if size > cfg.gone_threshold:\n",
    "                    status = 'GONE'\n",
    "                else:\n",
    "                    status = 'INACTIVE'\n",
    "                #start = (datetime.strptime(action_days[i], \"%Y-%m-%d\") + dt.timedelta(days=th)).strftime(\"%Y-%m-%d\")\n",
    "                #dates = start + '/' + action_days[i + 1]\n",
    "                dates = action_days[i] + '/' + action_days[i + 1]\n",
    "                util.add(period_detail, [size, dates, th, status, previously])\n",
    "            else:\n",
    "                break_start = period_detail.at[0, 'dates'].split('/')[0]\n",
    "                new_end = action_days[i + 1]\n",
    "                period_detail.at[0, 'len'] = util.daysBetween(break_start, new_end)  # New size\n",
    "                period_detail.at[0, 'dates'] = break_start + '/' + new_end  # New dates\n",
    "                # Same th\n",
    "                # Same status\n",
    "                # Same previously\n",
    "        else:  # (status=='NCUT')\n",
    "            diff = util.daysBetween(action_days[i], action_days[i + 1])\n",
    "            size = util.daysBetween(period_start, action_days[i + 1])\n",
    "            if size > th:\n",
    "                residual = size - (th + 1)\n",
    "                if residual > th:\n",
    "                    # The sub-break is actually made of 2 breaks: Non-coding + Inactive/Gone\n",
    "                    status = 'NON_CODING'\n",
    "                    final_date = (datetime.strptime(period_start, \"%Y-%m-%d\") + timedelta(days=(th + 1))).strftime(\"%Y-%m-%d\")\n",
    "                    dates = period_start + '/' + final_date\n",
    "                    actual_size = util.daysBetween(period_start, final_date)\n",
    "                    util.add(period_detail, [actual_size, dates, th, status, previously])\n",
    "\n",
    "                    previously = status\n",
    "                    if residual > cfg.gone_threshold:\n",
    "                        status = 'GONE'\n",
    "                    else:\n",
    "                        status = 'INACTIVE'\n",
    "                    dates = final_date + '/' + action_days[i + 1]\n",
    "                    second_size = util.daysBetween(final_date, action_days[i + 1])\n",
    "                    util.add(period_detail, [second_size, dates, th, status, previously])\n",
    "                else:\n",
    "                    # The sub-break becomes a Non-coding\n",
    "                    status = 'NON_CODING'\n",
    "                    dates = period_start + '/' + action_days[i + 1]\n",
    "                    actual_size = util.daysBetween(period_start, action_days[i + 1])\n",
    "                    util.add(period_detail, [actual_size, dates, th, status, previously])\n",
    "    # A Final status 'INACTIVE', 'GONE' or 'NCUT' means an UNFREEZING ('NCUT' is not written into the detail list)\n",
    "\n",
    "    last_end = period_detail.at[0, 'dates'].split('/')[1]\n",
    "    if status == 'NCUT':\n",
    "        if last_end == cfg.data_collection_date:\n",
    "            status = cfg.NC\n",
    "            start = period_detail.at[0, 'dates'].split('/')[1]\n",
    "            size = util.daysBetween(start, last_end)\n",
    "            util.add(period_detail, [size, start+'/'+last_end, th, status, previously])\n",
    "        else:\n",
    "            status = previously\n",
    "            break_start = period_detail.at[0, 'dates'].split('/')[0]\n",
    "            new_end = action_days[i + 1]\n",
    "            period_detail.at[0, 'len'] = util.daysBetween(break_start, new_end)  # New size\n",
    "            period_detail.at[0, 'dates'] = break_start + '/' + new_end  # New dates\n",
    "            last_end = new_end\n",
    "            # Same th\n",
    "            # Same status\n",
    "            # Same previously\n",
    "    if last_end == cfg.data_collection_date:\n",
    "        period_detail.at[0, 'label'] += '(NOW)'\n",
    "    else:\n",
    "        util.add(period_detail, [0, last_end, 0, 'ACTIVE', status])\n",
    "    return period_detail\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cf2129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lable_breaks_transformer(labeled_breaks);\n",
    "    start_date = \n",
    "    end_date = \n",
    "    len_days = (end_date - start_date).days + 1\n",
    "    \n",
    "    df = pandas.DataFrame(columns=['day', 'state', '', 'previously'])\n",
    "    for i in range(len_days):\n",
    "\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e985bf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #identifyInactivityPeriods\n",
    "    #\"Resources/repositories.txt\"\n",
    "    repos_file= '../' + cfg.repos_file\n",
    "    #\"../Organizations\"\n",
    "    organizationFolder = cfg.main_folder\n",
    "\n",
    "\n",
    "    #identifyBreaks\n",
    "    win = cfg.sliding_window_size\n",
    "    shift = cfg.shift\n",
    "\n",
    "    with open(repos_file) as f:\n",
    "        repos_file = f.readlines()\n",
    "        for repo in repos_file:\n",
    "            #take the end '\\n' out\n",
    "            repo = repo.rstrip('\\n')\n",
    "            organization, project = repo.split('/')\n",
    "\n",
    "            print(f\"Start Identifying inactivity periods for {organization}/{project}...\")\n",
    "\n",
    "            authors, pauses = identifyInactivityPeriods( organizationFolder, organization, project)\n",
    "            pauses_list = pauses.values.tolist()\n",
    "            print(f\"Finihsed 1st step in identifying inactivity periods for {len(authors)} developers\")\n",
    "\n",
    "            output_folder = organizationFolder + '/' + repo + \"/Results\"\n",
    "            os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "            for dev in authors:\n",
    "                print(f\"Started identifing breaks for {dev} in {organization}/{project}\")\n",
    "                breaks_df = identifyBreaks(pauses_list, developer=dev, window=win, shift=shift, debug_folder=output_folder )\n",
    "                breaks_df.to_csv(os.path.join(output_folder, f\"{dev}_breaks.csv\"),\n",
    "                                 \n",
    "                sep=cfg.CSV_separator, na_rep=cfg.CSV_missing, index=False, lineterminator=\"\\n\")\n",
    "                \n",
    "\n",
    "                # 1)  load or build the ACTIONS table -------------------------------\n",
    "                workingFolder = os.path.join(organizationFolder, repo)\n",
    "                \n",
    "                actions_path = Path(workingFolder) / f\"{dev}_actions_table.csv\"\n",
    "\n",
    "\n",
    "                if actions_path.is_file():\n",
    "                    user_actions = pandas.read_csv(actions_path, sep=cfg.CSV_separator, index_col=0)\n",
    "                else:\n",
    "                    user_actions = get_activities(workingFolder, dev)\n",
    "\n",
    "\n",
    "                # 2)  find dev's break -----------------------------------------------\n",
    "\n",
    "                labeled_breaks = pandas.DataFrame(columns=['len', 'dates', 'th', 'label', 'previously'])\n",
    "                for i, b in breaks_df.iterrows():\n",
    "                    # CHECK ACTIVITIES\n",
    "                    break_duration = b['len']\n",
    "                    break_dates = b['dates']\n",
    "                    threshold = b['th']\n",
    "                    break_range = break_dates.split('/')\n",
    "                    inner_start = (datetime.strptime(break_range[0], \"%Y-%m-%d\") + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "                    inner_end = (datetime.strptime(break_range[1], \"%Y-%m-%d\") - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "                    break_actions = user_actions.loc[:, inner_start:inner_end]  # Gets only the chosen period\n",
    "                    break_actions = break_actions.loc[~(break_actions == 0).all(axis=1)]  # Removes the actions not performed\n",
    "                    # print a debug message for break_actions\n",
    "\n",
    "                    is_activity_day = (break_actions != 0).any()  # List Of Columns With at least a Non-Zero Value\n",
    "                    action_days = is_activity_day.index[is_activity_day].tolist()  # List Of Columns NAMES Having Column Names at least a Non-Zero Value\n",
    "                    print(\"action_days:\",action_days)\n",
    "                    if len(break_actions) > 0:  # There are other activities: the Break is Non-coding\n",
    "                        break_detail = splitBreak(break_dates, action_days, threshold)\n",
    "                        # Exclude columns where all entries are NA\n",
    "                        labeled_breaks = labeled_breaks.dropna(axis=1, how='all')\n",
    "                        break_detail = break_detail.dropna(axis=1, how='all')\n",
    "                        # Concatenate DataFrames\n",
    "                        labeled_breaks = pandas.concat([labeled_breaks, break_detail], ignore_index=True)\n",
    "                    else:  # No other activities: the Break is Inactive or Gone\n",
    "                        print(f\"Break {break_dates} for {dev} in {organization}/{project} is inactive or gone, no further processing needed.\")\n",
    "                out_csv = Path(output_folder) / f\"{dev}_labeled_breaks.csv\"\n",
    "                labeled_breaks.to_csv(out_csv,\n",
    "                                      sep=cfg.CSV_separator, na_rep=cfg.CSV_missing, index=False, quoting=None, lineterminator='\\n')\n",
    "\n",
    "                #for every file in the breaks folder\n",
    "                print(\"✓ wrote output →            :\", out_csv)\n",
    "\n",
    "                # make feature set\n",
    "                \n",
    "                \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45972247",
   "metadata": {},
   "source": [
    "# END OF THE TEST CODE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401a54c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from github import Github\n",
    "\n",
    "secrets = [\n",
    "\n",
    "]\n",
    "for new_token in secrets:\n",
    "    ghub = Github(new_token)\n",
    "    search_limit = ghub.get_rate_limit().search.remaining\n",
    "    core_limit = ghub.get_rate_limit().core.remaining\n",
    "    reset = ghub.get_rate_limit().core.reset\n",
    "    #change the time to be in Mountain Standard Time (MST) \n",
    "    reset = reset.astimezone(tz=None)  # Convert to local timezone\n",
    "    # Print the limits\n",
    "\n",
    "    print(f\"Search limit for token {new_token}:\\n {search_limit}, {core_limit}, {reset} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93491cd0",
   "metadata": {},
   "source": [
    "### Find Core Devs TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "705da543",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new\n",
    "def findCoreDevelopers(\n",
    "    url: str,\n",
    "    dest_root: str | Path = \".tf_cache\",\n",
    "    *,\n",
    "    name: str | None = None,\n",
    "    branch: str | None = None,\n",
    "    refresh: bool = False,\n",
    ") -> tuple[list[str], list[str]]:        # <- correct annotation\n",
    "    \"\"\"\n",
    "    Clone <url> (or reuse/refresh an existing clone) and run Truck‑Factor.\n",
    "    Returns (authors, emails) – two parallel lists with the same length.\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # Paths\n",
    "    # ------------------------------------------------------------------ #\n",
    "    org, repo = url.rstrip(\"/\").split(\"/\")[-2:]\n",
    "    repo = repo.removesuffix(\".git\")\n",
    "    repo_path   = Path(cfg.main_folder) / org / repo          # actual repo\n",
    "    cache_dir   = repo_path / dest_root                       # .tf_cache\n",
    "    tf_csv      = cache_dir / \"TruckFactor.csv\"\n",
    "\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # 1. Return cached result if possible\n",
    "    # ------------------------------------------------------------------ #\n",
    "    if tf_csv.is_file() and not refresh:\n",
    "        cache_df = pandas.read_csv(\n",
    "            tf_csv,\n",
    "            sep=cfg.CSV_separator,\n",
    "            encoding=\"utf-8\",\n",
    "        )\n",
    "        return (\n",
    "            cache_df[\"login\"].tolist(),\n",
    "            cache_df[\"email\"].tolist(),\n",
    "        )\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # 2. Ensure we have a local clone\n",
    "    # ------------------------------------------------------------------ #\n",
    "    try:\n",
    "        if (cache_dir / \".git\").is_dir():\n",
    "            repo = Repo(cache_dir)\n",
    "            if refresh:\n",
    "                repo.git.fetch(\"--all\", \"--prune\")\n",
    "            if branch:\n",
    "                repo.git.checkout(branch)\n",
    "                if refresh:\n",
    "                    repo.git.pull()\n",
    "        else:\n",
    "            # Empty dir or non‑existent – clone afresh\n",
    "            if cache_dir.exists():\n",
    "                shutil.rmtree(cache_dir, ignore_errors=True)\n",
    "            repo = Repo.clone_from(url, cache_dir, branch=branch)\n",
    "    except git_exc.GitCommandError as e:\n",
    "        raise RuntimeError(f\"Git failed: {e.stderr or e}\") from e\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # 3. Compute Truck Factor (this calls your patched compute_tf)\n",
    "    # ------------------------------------------------------------------ #\n",
    "    tf, critical_sha, authors, emails = compute_tf(str(cache_dir))\n",
    "\n",
    "    # Always lists from here on\n",
    "    authors = list(authors)\n",
    "    emails  = list(emails)\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # 4. Cache the result for next time\n",
    "    # ------------------------------------------------------------------ #\n",
    "    pandas.DataFrame({\"login\": authors, \"email\": emails}).to_csv(\n",
    "        tf_csv,\n",
    "        sep=cfg.CSV_separator,\n",
    "        index=False,\n",
    "        lineterminator=\"\\n\",\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "\n",
    "    return authors, emails\n",
    "#old\n",
    "def findCoreDevelopers(\n",
    "    url: str,\n",
    "    dest_root: str | Path = \".tf_cache\",\n",
    "    *,\n",
    "    name: str | None = None,\n",
    "    branch: str | None = None,\n",
    "    refresh: bool = False,\n",
    ") -> tuple[int, str, list[str]]:\n",
    "    \"\"\"\n",
    "    Clone <url> (or reuse/refresh an existing clone) and run Truck-Factor.\n",
    "    Returns (tf, critical_sha, authors).\n",
    "    \"\"\"\n",
    "    # --------------------------------------------------------------------- #\n",
    "    dest_root = Path(dest_root).expanduser().resolve()    # .../rails/rails\n",
    "    name = name or url.rstrip(\"/\").split(\"/\")[-1].removesuffix(\".git\")\n",
    "    dest = dest_root / name\n",
    "    tf_cache  = dest / \".tf_cache\"\n",
    "    tf_cache.mkdir(parents=True, exist_ok=True)                  \n",
    "    tf_csv = dest / \"TruckFactor.csv\"\n",
    "    \n",
    "    clone_path = dest / name                          # .../.tf_cache/rails\n",
    "\n",
    "    if tf_csv.is_file():\n",
    "        logging.info(\"TF cache hit – using %s\", tf_csv)\n",
    "        return pandas.read_csv(tf_csv, encoding=\"utf-8\")[\"login\"].tolist()\n",
    "\n",
    "    \n",
    "\n",
    "    repo = None\n",
    "    # --------------------------------------------------------------------- #\n",
    "    try:\n",
    "        if clone_path.exists():\n",
    "            try:\n",
    "                repo = Repo(clone_path)\n",
    "            except git_exc.InvalidGitRepositoryError:\n",
    "                # Directory exists but isn't a repo – start fresh\n",
    "                shutil.rmtree(dest, ignore_errors=True)\n",
    "                repo = Repo.clone_from(url, to_path=dest, branch=branch)\n",
    "            else:\n",
    "                # Repo is valid – refresh if asked\n",
    "                if refresh:\n",
    "                    repo.git.fetch(\"--all\", \"--prune\")\n",
    "                if branch:\n",
    "                    repo.git.checkout(branch)\n",
    "                    if refresh:\n",
    "                        repo.git.pull()\n",
    "        else:\n",
    "            repo = Repo.clone_from(url, clone_path, branch=branch)\n",
    "    except git_exc.GitCommandError as e:\n",
    "        raise RuntimeError(f\"Git failed: {e.stderr or e}\") from e\n",
    "\n",
    "    # --------------------------------------------------------------------- #\n",
    "    # Ensure the repo is NOT empty (at least one commit reachable)\n",
    "    if not list(repo.iter_commits('--all', max_count=1)):\n",
    "        # Something went wrong – start over with a clean clone\n",
    "        shutil.rmtree(dest, ignore_errors=True)\n",
    "        repo = Repo.clone_from(url, clone_path, branch=branch)\n",
    "\n",
    "    # --------------------------------------------------------------------- #\n",
    "    # Truck-Factor\n",
    "    #if the truck factor file does not exist, we compute it\n",
    "    if not tf_csv.is_file():\n",
    "        print(\"Computing Truck Factor for\", clone_path)\n",
    "        tf, critical_sha, authors = compute_tf(str(clone_path))\n",
    "        \n",
    "    else:\n",
    "        print(\"Using cached Truck Factor from %s\", tf_csv)\n",
    "    \n",
    "\n",
    "    pandas.DataFrame(authors, columns=[\"login\"])\\\n",
    "      .to_csv(tf_csv ,\n",
    "              sep=cfg.CSV_separator,\n",
    "              index=False,\n",
    "              lineterminator=\"\\n\",\n",
    "              encoding=\"utf-8\")\n",
    "\n",
    "    return authors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS485",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
